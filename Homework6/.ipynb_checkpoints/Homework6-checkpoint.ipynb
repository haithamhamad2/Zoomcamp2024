{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6a7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, window, col\n",
    "from pyspark.sql import functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2408a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc9f15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = '35.197.53.150:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d3fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0745a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef98024",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'test-topic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a998412",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16049fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d14bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Sending took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Flush took {(t2 - t1):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce6d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on ssh run\n",
    "!docker exec -it redpanda-1 rpk topic consume test-topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9071d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70740bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One time run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget {url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff86d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip green_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e29712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create topic green-trips\n",
    "!docker exec -it redpanda-1 rpk topic create green-trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36909fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae02c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='green_tripdata_2019-10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78dede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = pd.read_csv(filename, usecols=['lpep_pickup_datetime','lpep_dropoff_datetime','PULocationID','DOLocationID','passenger_count','trip_distance','tip_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b65aebfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:26:02</td>\n",
       "      <td>2019-10-01 00:39:58</td>\n",
       "      <td>112</td>\n",
       "      <td>196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:18:11</td>\n",
       "      <td>2019-10-01 00:22:38</td>\n",
       "      <td>43</td>\n",
       "      <td>263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:09:31</td>\n",
       "      <td>2019-10-01 00:24:47</td>\n",
       "      <td>255</td>\n",
       "      <td>228</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:37:40</td>\n",
       "      <td>2019-10-01 00:41:49</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 00:08:13</td>\n",
       "      <td>2019-10-01 00:17:56</td>\n",
       "      <td>97</td>\n",
       "      <td>188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lpep_pickup_datetime lpep_dropoff_datetime  PULocationID  DOLocationID  \\\n",
       "0  2019-10-01 00:26:02   2019-10-01 00:39:58           112           196   \n",
       "1  2019-10-01 00:18:11   2019-10-01 00:22:38            43           263   \n",
       "2  2019-10-01 00:09:31   2019-10-01 00:24:47           255           228   \n",
       "3  2019-10-01 00:37:40   2019-10-01 00:41:49           181           181   \n",
       "4  2019-10-01 00:08:13   2019-10-01 00:17:56            97           188   \n",
       "\n",
       "   passenger_count  trip_distance  tip_amount  \n",
       "0              1.0           5.88        0.00  \n",
       "1              1.0           0.80        0.00  \n",
       "2              2.0           7.50        0.00  \n",
       "3              1.0           0.90        0.00  \n",
       "4              1.0           2.52        2.26  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec -it redpanda-1 rpk topic list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df9e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2493ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a75927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'green-trips'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa4436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending took 46.84 seconds\n"
     ]
    }
   ],
   "source": [
    "t3 = time.time()\n",
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    #print(row_dict)\n",
    "    #break\n",
    "    #producer.send(topic_name, json.dumps(row_dict).encode('utf-8'))\n",
    "    producer.send(topic_name, value=row_dict)\n",
    "    #value=row_dict)\n",
    "    #time.sleep(0.005)\n",
    "    # TODO implement sending the data here\n",
    "t4 = time.time()\n",
    "print(f'Sending took {(t4 - t3):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on ssh run\n",
    "!docker exec -it redpanda-1 rpk topic consume green-trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "230aa7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "t5 = time.time()\n",
    "producer.flush()\n",
    "t6 = time.time()\n",
    "print(f'Flushing took {(t6 - t5):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3afc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02a90f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/Haitham.hamad/spark/spark-3.3.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/Haitham.hamad/.ivy2/cache\n",
      "The jars for the packages stored in: /home/Haitham.hamad/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-56db6a8a-1a57-4dd9-b051-fcebc70c1d5e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 783ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-56db6a8a-1a57-4dd9-b051-fcebc70c1d5e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/15ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/19 11:23:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#import pyspark\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43ec9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #.option(\"startingOffsets\", \"earliest\") \\\n",
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", server) \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cf5eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/19 11:23:15 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-68139be4-0aad-4da4-9603-676c8dc5bb3b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/19 11:23:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:00:48\", \"lpep_dropoff_datetime\": \"2019-10-01 00:05:42\", \"PULocationID\": 210, \"DOLocationID\": 108, \"passenger_count\": 2.0, \"trip_distance\": 1.03, \"tip_amount\": 2.19}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 3, 19, 11, 21, 32, 136000), timestampType=0)\n"
     ]
    }
   ],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbdbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac5a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c939f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "878c3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = green_stream \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(F.from_json(\"value\", \"DOLocationID INT, PULocationID INT, lpep_pickup_datetime STRING, lpep_dropoff_datetime STRING, passenger_count INT, tip_amount DOUBLE, trip_distance DOUBLE\") \\\n",
    "    .alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969affbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the stream data (assuming the data has appropriate columns)\n",
    "#parsed_data = green_stream.selectExpr(\"CAST(value AS STRING) as raw_data\")\n",
    "parsed_df = green_stream.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(F.from_json(\"value\", \"DOLocationID INT\") \\\n",
    "    .alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fd53f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a timestamp column, group by timestamp and DOLocationID and order by count\n",
    "result_df = parsed_df \\\n",
    "    .withColumn(\"timestamp\", F.current_timestamp()) \\\n",
    "    .groupBy(F.window(\"timestamp\", \"5 minutes\"), \"DOLocationID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6948f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/19 11:23:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-dec1c93b-8062-4668-9c3a-0d5fbbe89996. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/19 11:23:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+------------+-----+\n",
      "|              window|DOLocationID|count|\n",
      "+--------------------+------------+-----+\n",
      "|{2024-03-19 11:20...|          74|17733|\n",
      "|{2024-03-19 11:20...|          42|15935|\n",
      "|{2024-03-19 11:20...|          41|14058|\n",
      "|{2024-03-19 11:20...|          75|12837|\n",
      "|{2024-03-19 11:20...|         129|11922|\n",
      "|{2024-03-19 11:20...|           7|11527|\n",
      "|{2024-03-19 11:20...|         166|10843|\n",
      "|{2024-03-19 11:20...|         236| 7913|\n",
      "|{2024-03-19 11:20...|         223| 7540|\n",
      "|{2024-03-19 11:20...|         238| 7318|\n",
      "|{2024-03-19 11:20...|          82| 7285|\n",
      "|{2024-03-19 11:20...|         181| 7279|\n",
      "|{2024-03-19 11:20...|          95| 7244|\n",
      "|{2024-03-19 11:20...|         244| 6731|\n",
      "|{2024-03-19 11:20...|          61| 6605|\n",
      "|{2024-03-19 11:20...|         116| 6337|\n",
      "|{2024-03-19 11:20...|         138| 6144|\n",
      "|{2024-03-19 11:20...|          97| 6049|\n",
      "|{2024-03-19 11:20...|          49| 5219|\n",
      "|{2024-03-19 11:20...|         151| 5152|\n",
      "+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the output to the console\n",
    "query = result_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef136bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the query to terminate\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the query to terminate or until the specified timeout\n",
    "query.awaitTermination(timeout=60)  # Wait for 60 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the streaming query gracefully\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75550074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the query is complete\n",
    "while query.isActive:\n",
    "    # Check if the query is still active\n",
    "    if not query.isActive:\n",
    "        break\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efaa7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20868cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "102be4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_green_stream = green_stream.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32fe4ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/19 11:25:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f5976c79-0048-498d-b35e-5811cec1d47c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/19 11:25:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "|lpep_pickup_datetime|lpep_dropoff_datetime|PULocationID|DOLocationID|passenger_count|trip_distance|tip_amount|\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "| 2019-10-01 00:00:48|  2019-10-01 00:05:42|         210|         108|            2.0|         1.03|      2.19|\n",
      "| 2019-10-01 00:45:08|  2019-10-01 01:04:28|          83|          36|            2.0|         5.75|       0.0|\n",
      "| 2019-10-01 00:32:44|  2019-10-01 00:46:53|          92|         260|            2.0|         6.01|      4.16|\n",
      "| 2019-10-01 00:05:09|  2019-10-01 00:18:34|          75|         119|            1.0|         1.08|      2.36|\n",
      "| 2019-10-01 00:18:16|  2019-10-01 00:53:16|          29|          92|            1.0|        21.39|       0.0|\n",
      "| 2019-10-01 00:03:02|  2019-10-01 00:09:54|         129|         129|            2.0|         2.23|       0.0|\n",
      "| 2019-10-01 00:48:33|  2019-10-01 00:53:42|          74|         168|            1.0|         1.24|      2.34|\n",
      "| 2019-10-01 00:03:09|  2019-10-01 00:28:01|          82|          75|            1.0|         8.19|       0.0|\n",
      "| 2019-10-01 00:02:26|  2019-10-01 00:12:24|          74|          69|            2.0|         2.28|       2.0|\n",
      "| 2019-10-01 00:25:15|  2019-10-01 00:47:25|         129|          28|            1.0|         9.22|       0.0|\n",
      "| 2019-10-01 00:44:27|  2019-10-01 00:47:17|         223|         223|            1.0|         0.86|      1.16|\n",
      "| 2019-10-01 00:08:52|  2019-10-01 00:59:25|          61|          18|            1.0|        18.82|       0.0|\n",
      "| 2019-10-01 00:39:21|  2019-10-01 00:44:12|          95|          95|            1.0|         1.03|       0.0|\n",
      "| 2019-10-01 00:17:46|  2019-10-01 00:28:49|         130|         205|            1.0|         3.69|      2.76|\n",
      "| 2019-10-01 01:02:12|  2019-10-01 01:06:15|         130|         197|            1.0|         1.21|      1.82|\n",
      "| 2019-10-01 00:24:10|  2019-10-01 00:42:01|         130|         145|            1.0|        10.34|      6.16|\n",
      "| 2019-10-01 00:17:49|  2019-10-01 00:33:39|           7|         157|            3.0|          3.1|       0.0|\n",
      "| 2019-10-01 00:46:07|  2019-10-01 00:51:22|         129|         129|            1.0|          1.1|       1.7|\n",
      "| 2019-10-01 00:08:33|  2019-10-01 00:22:10|          66|          79|            2.0|         5.08|       0.0|\n",
      "| 2019-10-01 00:47:09|  2019-10-01 00:49:58|          82|         129|            1.0|          0.6|       0.0|\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = parsed_green_stream \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3cfd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7b7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83a222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53756287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a9914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b9ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94759ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
